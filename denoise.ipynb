{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20230327.parquet...\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 67108864 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m output_path = Path(\u001b[33m\"\u001b[39m\u001b[33mstaypoints/staypoints_20230327.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m output_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mprocess_file\u001b[39m\u001b[34m(parquet_path, output_path)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_file\u001b[39m(parquet_path, output_path):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparquet_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeviceid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + df[\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m), \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm.\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m     df = df.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1831\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1832\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1833\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1834\u001b[39m         memory_map=memory_map, buffer_size=buffer_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1840\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1841\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1477\u001b[39m         index_columns = [\n\u001b[32m   1478\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1479\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1480\u001b[39m         ]\n\u001b[32m   1481\u001b[39m         columns = (\n\u001b[32m   1482\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1483\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\_dataset.pyx:574\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3865\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowMemoryError\u001b[39m: realloc of size 67108864 failed"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import tqdm\n",
    "\n",
    "# Fast vectorized haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # meters\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = np.sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "# Stay point extraction logic\n",
    "def extract_stay_points(df, D_thres=200, T_thres=timedelta(minutes=20)):\n",
    "    stay_points = []\n",
    "    for device_id, group in tqdm.tqdm(df.groupby('deviceid', observed=True), desc=\"Processing devices\"):\n",
    "        group = group.sort_values(\"datetime\")\n",
    "        points = group[[\"datetime\", \"lat\", \"lon\"]].values\n",
    "        i = 0\n",
    "        while i < len(points):\n",
    "            j = i + 1\n",
    "            while j < len(points):\n",
    "                dist = haversine(points[i][1], points[i][2], points[j][1], points[j][2])\n",
    "                if dist > D_thres:\n",
    "                    delta_t = points[j][0] - points[i][0]\n",
    "                    if delta_t > T_thres:\n",
    "                        lat_mean = group.iloc[i:j][\"lat\"].mean()\n",
    "                        lon_mean = group.iloc[i:j][\"lon\"].mean()\n",
    "                        stay_points.append({\n",
    "                            \"deviceid\": device_id,\n",
    "                            \"arrival_time\": points[i][0],\n",
    "                            \"leave_time\": points[j][0],\n",
    "                            \"stay_lat\": lat_mean,\n",
    "                            \"stay_lon\": lon_mean,\n",
    "                            \"duration_min\": delta_t.total_seconds() / 60\n",
    "                        })\n",
    "                    break\n",
    "                j += 1\n",
    "            i = j\n",
    "    return pd.DataFrame(stay_points)\n",
    "\n",
    "# Entry point: process a file\n",
    "def process_file(parquet_path, output_path):\n",
    "    print(f\"Reading {parquet_path.name}...\")\n",
    "    df = pd.read_parquet(parquet_path, columns=[\"deviceid\", \"date\", \"time\", \"lon\", \"lat\"])\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str), format=\"%d.%m.%Y %H:%M:%S\")\n",
    "    df = df.drop(columns=[\"date\", \"time\"])\n",
    "\n",
    "    stay_df = extract_stay_points(df)\n",
    "    stay_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {len(stay_df)} stay points to {output_path.name}\")\n",
    "\n",
    "# Run this part per file\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = Path(\"data/20230327.parquet\")\n",
    "    output_path = Path(\"staypoints/staypoints_20230327.parquet\")\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    process_file(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line_from_to(initial:list[int, int], final:list[int, int], m:folium.Map):\n",
    "    # Use the folium AntPath plugin to draw a line between two points\n",
    "    AntPath(\n",
    "        locations=[initial, final],\n",
    "        dash_array=[20, 20],\n",
    "        delay=1000,\n",
    "        color=\"#A00000\",\n",
    "        pulse_color=\"#A00000\",\n",
    "        weight=5,\n",
    "        tooltip=\"From start to finish\"\n",
    "    ).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stay_points = \u001b[43mpd\u001b[49m.read_parquet(\u001b[33m'\u001b[39m\u001b[33mstay_points.parquet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m display(stay_points)\n\u001b[32m      3\u001b[39m unique_devices = stay_points[\u001b[33m'\u001b[39m\u001b[33mdeviceid\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "stay_points = dd.read_parquet('stay_points.parquet')\n",
    "display(stay_points)\n",
    "unique_devices = stay_points['deviceid'].unique()\n",
    "unique_devices = unique_devices[:10]  # Limit to 10 devices for demonstration\n",
    "os.makedirs('maps', exist_ok=True)\n",
    "for device in unique_devices:\n",
    "    device_df = stay_points[stay_points['deviceid'] == device]\n",
    "    m = folium.Map(location=[device_df['stay_lat'].mean(), device_df['stay_lon'].mean()], zoom_start=12)\n",
    "    for i in range(len(device_df)-1):\n",
    "        initial = (device_df.iloc[i]['stay_lat'], device_df.iloc[i]['stay_lon'])\n",
    "        final = (device_df.iloc[i+1]['stay_lat'], device_df.iloc[i+1]['stay_lon'])\n",
    "        add_line_from_to(initial, final, m)\n",
    "    m.save(f'maps/{device}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing days:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Automatically find all .parquet files in the data folder\n",
    "days = sorted([\"data/\" + f for f in os.listdir(\"data\") if f.endswith(\".parquet\")])\n",
    "\n",
    "for day in tqdm.tqdm(days, desc=\"Processing days\"):\n",
    "    current = dd.read_parquet(day)\n",
    "    # Randomly sample 10% of the data following the same distribution as the original data\n",
    "    sample = current.sample(frac=0.1, random_state=42)\n",
    "    # Save the sampled data to a new parquet file\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(\"sampled_data\", exist_ok=True)\n",
    "    sample.to_parquet(day.replace(\"data/\", \"sampled_data/\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_sample = sorted([\"sampled_data/\" + f for f in os.listdir(\"sampled_data\") if f.endswith(\".parquet\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
