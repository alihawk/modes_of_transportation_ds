{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Parquet files in data_binned\n"
     ]
    }
   ],
   "source": [
    "IN_DIR = Path(\"data_binned\") \n",
    "OUT_DIR = Path(\"data_transitions\")\n",
    "\n",
    "parquet_files = list(IN_DIR.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(parquet_files)} Parquet files in {IN_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 20230331.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (87260420, 12)\n",
      "Sample data:\n",
      "   deviceid        date      time       lon        lat            datetime  \\\n",
      "0         0  31.03.2023  02:29:54  14.52146  46.052380 2023-03-31 02:29:54   \n",
      "1         0  31.03.2023  07:40:40  14.52137  46.051311 2023-03-31 07:40:40   \n",
      "2         0  31.03.2023  07:41:13  14.52137  46.051311 2023-03-31 07:41:13   \n",
      "3         0  31.03.2023  08:00:11  14.52139  46.053879 2023-03-31 08:00:11   \n",
      "4         0  31.03.2023  08:00:14  14.52139  46.053879 2023-03-31 08:00:14   \n",
      "\n",
      "   device_change      dist_m     dt  speed_m_s  zone_id  time_bin  \n",
      "0          False  119.063995   2218   0.053681     1133         2  \n",
      "1          False  119.063995  18646   0.006385     1133         7  \n",
      "2          False   30.052505     30   1.001750     1133         7  \n",
      "3          False  285.569763   1138   0.250940     1827         8  \n",
      "4          False    0.000000      3   0.000000     1827         8  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 344\u001b[0m\n\u001b[1;32m    338\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# Perform any necessary operations on df here\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m#transition_matrices_per_day.append(create_zone_transition_matrix_approach1(df))\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m#df = create_zone_transitions_sequential_approach1(df)\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m df \u001b[38;5;241m=\u001b[39m create_zone_transitions_sequential_approach2(df)\n\u001b[1;32m    346\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to add to the df FROM/TO: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 239\u001b[0m, in \u001b[0;36mcreate_zone_transitions_sequential_approach2\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;66;03m# Set transition on the last row of the current time bin\u001b[39;00m\n\u001b[1;32m    238\u001b[0m             result_df\u001b[38;5;241m.\u001b[39mloc[result_df\u001b[38;5;241m.\u001b[39mindex[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFROM\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m from_zone\n\u001b[0;32m--> 239\u001b[0m             result_df\u001b[38;5;241m.\u001b[39mloc[result_df\u001b[38;5;241m.\u001b[39mindex[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTO\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m to_zone\n\u001b[1;32m    240\u001b[0m             result_df\u001b[38;5;241m.\u001b[39mloc[result_df\u001b[38;5;241m.\u001b[39mindex[i], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexing.py:814\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m--> 814\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    817\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexing.py:697\u001b[0m, in \u001b[0;36m_LocationIndexer._get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;66;03m# suppress \"Too many indexers\"\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_tuple(key)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mrange\u001b[39m):\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# GH#45479 test_loc_setitem_range_key\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexing.py:896\u001b[0m, in \u001b[0;36m_LocationIndexer._convert_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# Note: we assume _tupleize_axis_indexer has been called, if necessary.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key_length(key)\n\u001b[0;32m--> 896\u001b[0m     keyidx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_indexer(k, axis\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key)]\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(keyidx)\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexing.py:896\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;66;03m# Note: we assume _tupleize_axis_indexer has been called, if necessary.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key_length(key)\n\u001b[0;32m--> 896\u001b[0m     keyidx \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_to_indexer(k, axis\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key)]\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(keyidx)\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexing.py:1366\u001b[0m, in \u001b[0;36m_LocIndexer._convert_to_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex) \u001b[38;5;129;01mand\u001b[39;00m is_hashable(key)):\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Otherwise get_loc will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m \n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;66;03m# if we are a label return me\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex):\n",
      "File \u001b[0;32m/cvmfs/sling.si/modules/el7/software/Anaconda3/2023.07-2/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m casted_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_indexer(key)\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def calculate_nan_percentages(df):\n",
    "    \"\"\"\n",
    "    Calculate and print the percentage of rows where VALID is True vs False.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with a 'VALID' column\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Count valid and invalid rows\n",
    "    valid_rows = df['VALID'].sum()  # Sum of True values\n",
    "    invalid_rows = total_rows - valid_rows\n",
    "    \n",
    "    # Calculate percentages\n",
    "    valid_percent = (valid_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "    invalid_percent = (invalid_rows / total_rows) * 100 if total_rows > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Valid rows (VALID=True): {valid_rows} ({valid_percent:.2f}%)\")\n",
    "    print(f\"Invalid rows (VALID=False): {invalid_rows} ({invalid_percent:.2f}%)\")\n",
    "\n",
    "\n",
    "def save_transitions_from_df(df, output_file):\n",
    "    \"\"\"\n",
    "    Save transition data directly from a dataframe with FROM, TO and VALID columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with FROM, TO and VALID columns\n",
    "        output_file: Path to save the file\n",
    "        format: 'parquet'\n",
    "        \n",
    "        how it looks like:\n",
    "        time_bin  FROM      TO  count\n",
    "0         0       19.0    19.0     68\n",
    "1         0       19.0   787.0      1\n",
    "2         0       19.0  1435.0      2\n",
    "...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to only rows with valid transitions\n",
    "    valid_df = df[df['VALID'] == True].copy()\n",
    "    \n",
    "    # Group by time_bin, FROM, TO and count\n",
    "    transitions_df = valid_df.groupby(['time_bin', 'FROM', 'TO']).size().reset_index(name='count')\n",
    "\n",
    "    transitions_df.to_parquet(output_file, compression='snappy')\n",
    "    # Print statistics\n",
    "    file_size = os.path.getsize(output_file) / (1024*1024)\n",
    "    \n",
    "    print(f\"Saved transitions to {output_file}\")\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    print(f\"Total transitions saved: {len(transitions_df)}\")\n",
    "    print(f\"Unique time bins: {len(transitions_df['time_bin'].unique())}\")\n",
    "    print(f\"Unique FROM zones: {len(transitions_df['FROM'].unique())}\")\n",
    "    print(f\"Unique TO zones: {len(transitions_df['TO'].unique())}\")\n",
    "    \n",
    "    return transitions_df\n",
    "\n",
    "def create_transition_hashmap(df):\n",
    "    \"\"\"\n",
    "    Create a transition matrix as a nested dictionary (hash map) from the dataframe.\n",
    "    Structure will be: result[time_bin][from_zone][to_zone] = count\n",
    "    \"\"\"\n",
    "    # Initialize the transition hash map\n",
    "    # Structure: transition_map[time_bin][from_zone][to_zone] = count\n",
    "    transition_map = {}\n",
    "    \n",
    "    # Filter to include only valid transitions\n",
    "    valid_df = df[df['VALID'] == True].copy()\n",
    "    \n",
    "    # Print basic stats\n",
    "    total_rows = len(df)\n",
    "    valid_rows = len(valid_df)\n",
    "    print(f\"Total rows in dataframe: {total_rows}\")\n",
    "    print(f\"Valid transitions: {valid_rows} ({valid_rows/total_rows*100:.2f}% of total)\")\n",
    "    \n",
    "    # Process each valid row\n",
    "    for _, row in valid_df.iterrows():\n",
    "        from_zone = row['FROM']  # Using the FROM column\n",
    "        to_zone = row['TO']      # Using the TO column\n",
    "        time_bin = row['time_bin']\n",
    "        \n",
    "        # Initialize nested dictionaries if they don't exist\n",
    "        if time_bin not in transition_map:\n",
    "            transition_map[time_bin] = {}\n",
    "        \n",
    "        if from_zone not in transition_map[time_bin]:\n",
    "            transition_map[time_bin][from_zone] = {}\n",
    "        \n",
    "        # Increment the transition count\n",
    "        if to_zone in transition_map[time_bin][from_zone]:\n",
    "            transition_map[time_bin][from_zone][to_zone] += 1\n",
    "        else:\n",
    "            transition_map[time_bin][from_zone][to_zone] = 1\n",
    "    \n",
    "    # Print transition statistics by time bin\n",
    "    print(\"\\nTransitions by time bin:\")\n",
    "    for time_bin in sorted(transition_map.keys()):\n",
    "        time_bin_transitions = sum(sum(counts.values()) for counts in transition_map[time_bin].values())\n",
    "        print(f\"  Time bin {time_bin}: {time_bin_transitions} transitions ({time_bin_transitions/valid_rows*100:.2f}% of valid)\")\n",
    "    \n",
    "    return transition_map\n",
    "\n",
    "def print_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate and print the percentage of valid and invalid transitions\n",
    "    using the VALID column.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Count valid transitions (VALID == True)\n",
    "    valid_transitions = df['VALID'].sum()\n",
    "    valid_pct = (valid_transitions / total_rows) * 100\n",
    "\n",
    "    # Count invalid transitions (VALID == False)\n",
    "    invalid_transitions = total_rows - valid_transitions\n",
    "    invalid_pct = (invalid_transitions / total_rows) * 100\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Valid transitions: {valid_transitions} ({valid_pct:.2f}%)\")\n",
    "    print(f\"Invalid transitions: {invalid_transitions} ({invalid_pct:.2f}%)\")\n",
    "\n",
    "    \n",
    "def create_zone_transitions_sequential_approach2(df):\n",
    "    \"\"\"\n",
    "    Create zone transitions based on the zone where users spend the most time in each time bin.\n",
    "    \n",
    "    For each device:\n",
    "    1. Calculate seconds spent in each zone within each time bin\n",
    "    2. Create transitions from the most frequent zone in time_bin to most frequent zone in time_bin+1\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with zone_id, time_bin, device_change columns and TIME column in HH:MM:SS format\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with FROM, TO and VALID columns added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Convert TIME to seconds since midnight for easier calculation\n",
    "    if 'time' in result_df.columns:\n",
    "        result_df['seconds'] = result_df['time'].apply(\n",
    "            lambda t: int(t.split(':')[0]) * 3600 + int(t.split(':')[1]) * 60 + int(t.split(':')[2])\n",
    "            if isinstance(t, str) else t.hour * 3600 + t.minute * 60 + t.second\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"DataFrame must have a TIME column in HH:MM:SS format\")\n",
    "    \n",
    "    # Initialize columns for result\n",
    "    result_df['FROM'] = np.nan\n",
    "    result_df['TO'] = np.nan\n",
    "    result_df['VALID'] = False\n",
    "    \n",
    "    # Extract arrays for faster processing\n",
    "    zone_ids = result_df[\"zone_id\"].to_numpy()\n",
    "    time_bins = result_df[\"time_bin\"].to_numpy()\n",
    "    seconds = result_df[\"seconds\"].to_numpy()\n",
    "    device_changes = result_df[\"device_change\"].to_numpy()\n",
    "    \n",
    "    # Create a device_id array if the column exists, otherwise create one based on device_change\n",
    "    if 'device_id' in result_df.columns:\n",
    "        device_ids = result_df[\"device_id\"].to_numpy()\n",
    "    else:\n",
    "        # Create synthetic device IDs based on device_change markers\n",
    "        device_ids = np.zeros(len(result_df), dtype=int)\n",
    "        current_id = 0\n",
    "        for i in range(len(result_df)):\n",
    "            if i > 0 and device_changes[i]:\n",
    "                current_id += 1\n",
    "            device_ids[i] = current_id\n",
    "    \n",
    "    # Create arrays for the next row's values\n",
    "    next_seconds = np.roll(seconds, -1)\n",
    "    next_device_changes = np.roll(device_changes, -1)\n",
    "    \n",
    "    # Mark the last row as a device change\n",
    "    next_device_changes[-1] = True\n",
    "    \n",
    "    # Calculate time differences between consecutive points (in seconds)\n",
    "    time_diffs = next_seconds - seconds\n",
    "    \n",
    "    # Zero out time differences at device changes or if too large (> 3600 seconds = 1 hour)\n",
    "    time_diffs[(next_device_changes) | (time_diffs > 3600)] = 0\n",
    "    \n",
    "    # Dictionary to store time spent in each zone by device and time bin\n",
    "    # Structure: {(device_id, time_bin): {zone_id: seconds}}\n",
    "    zone_times = {}\n",
    "    \n",
    "    # Process rows sequentially\n",
    "    for i in range(len(result_df)):\n",
    "        # Get current device ID from the array\n",
    "        device_id = device_ids[i]\n",
    "        time_bin = time_bins[i]\n",
    "        zone_id = zone_ids[i]\n",
    "        \n",
    "        # Skip last row of each device (no time diff available)\n",
    "        if i < len(result_df) - 1 and not next_device_changes[i]:\n",
    "            # Initialize counter if needed\n",
    "            key = (device_id, time_bin)\n",
    "            if key not in zone_times:\n",
    "                zone_times[key] = {}\n",
    "            if zone_id not in zone_times[key]:\n",
    "                zone_times[key][zone_id] = 0\n",
    "            \n",
    "            # Add time spent in this zone\n",
    "            zone_times[key][zone_id] += time_diffs[i]\n",
    "    \n",
    "    # Find primary zone for each device and time bin\n",
    "    primary_zones = {}  # {(device_id, time_bin): primary_zone_id}\n",
    "    \n",
    "    for key, zones in zone_times.items():\n",
    "        if zones:\n",
    "            primary_zones[key] = max(zones.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    # Create transitions between consecutive time bins for each device\n",
    "    for i in range(len(result_df) - 1):\n",
    "        # Skip if this is the last row for a device\n",
    "        if next_device_changes[i]:\n",
    "            continue\n",
    "        \n",
    "        device_id = device_ids[i]\n",
    "        current_bin = time_bins[i]\n",
    "        next_bin = time_bins[i+1]\n",
    "        \n",
    "        # Create transition only if time bins are consecutive and for the same device\n",
    "        if not next_device_changes[i] and next_bin == current_bin + 1:\n",
    "            current_key = (device_id, current_bin)\n",
    "            next_key = (device_id, next_bin)\n",
    "            \n",
    "            # Only create transition if we have primary zones for both bins\n",
    "            if current_key in primary_zones and next_key in primary_zones:\n",
    "                from_zone = primary_zones[current_key]\n",
    "                to_zone = primary_zones[next_key]\n",
    "                \n",
    "                # Set transition on the last row of the current time bin\n",
    "                result_df.loc[result_df.index[i], 'FROM'] = from_zone\n",
    "                result_df.loc[result_df.index[i], 'TO'] = to_zone\n",
    "                result_df.loc[result_df.index[i], 'VALID'] = True\n",
    "    \n",
    "    # Clean up\n",
    "    if 'seconds' in result_df.columns:\n",
    "        result_df = result_df.drop('seconds', axis=1)\n",
    "    \n",
    "    # Add the same_zone column for convenience\n",
    "    result_df[\"same_zone\"] = (result_df[\"FROM\"] == result_df[\"TO\"]) & result_df[\"VALID\"]\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def create_zone_transitions_sequential_approach1(df):\n",
    "    # Extract arrays from dataframe\n",
    "    zone_ids = df[\"zone_id\"].to_numpy()\n",
    "    time_bins = df[\"time_bin\"].to_numpy()\n",
    "    dc = df[\"device_change\"].to_numpy()\n",
    "    \n",
    "    # Create arrays for the next row's values using roll\n",
    "    zone_next = np.roll(zone_ids, -1)\n",
    "    time_bin_next = np.roll(time_bins, -1)\n",
    "    \n",
    "    # Handle device changes - mark the last row of each device\n",
    "    last_row = np.roll(dc, -1)\n",
    "    last_row[-1] = True  # Last row of the entire dataframe\n",
    "    \n",
    "    # Valid transitions are when:\n",
    "    # 1. Not at a device change boundary\n",
    "    # 2. Time bins are the same\n",
    "    valid_idx = (~last_row) & (time_bins == time_bin_next)\n",
    "    \n",
    "    # Create FROM and TO columns (use NaN for invalid transitions)\n",
    "    from_zones = np.full(len(df), np.nan, dtype=float)\n",
    "    to_zones = np.full(len(df), np.nan, dtype=float)\n",
    "    \n",
    "    # Set values only for valid transitions\n",
    "    from_zones[valid_idx] = zone_ids[valid_idx]\n",
    "    to_zones[valid_idx] = zone_next[valid_idx]\n",
    "    \n",
    "    # Add columns to dataframe\n",
    "    df[\"FROM\"] = from_zones\n",
    "    df[\"TO\"] = to_zones\n",
    "    \n",
    "    # Add VALID column - TRUE when both FROM and TO are not NaN\n",
    "    # This is equivalent to valid_idx\n",
    "    df[\"VALID\"] = valid_idx\n",
    "    \n",
    "    # Add the same_zone column for convenience\n",
    "    df[\"same_zone\"] = (zone_ids == zone_next) & valid_idx\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_zone_transition_matrix_approach1(df):\n",
    "    # Get unique zone_ids and time_bins\n",
    "    unique_zones = df['zone_id'].unique()\n",
    "    time_bins = df['time_bin'].unique()\n",
    "    \n",
    "    # Initialize transition matrices for each time_bin\n",
    "    transition_matrices = {tb: pd.DataFrame(0, index=unique_zones, columns=unique_zones) \n",
    "                          for tb in time_bins}\n",
    "    \n",
    "    # Iterate through rows sequentially\n",
    "    for i in range(len(df) - 1):\n",
    "        current_row = df.iloc[i]\n",
    "        next_row = df.iloc[i + 1]\n",
    "        \n",
    "        # Check if we're still tracking the same device and in the same time bin\n",
    "        if (not next_row['device_change'] and \n",
    "            current_row['time_bin'] == next_row['time_bin']):\n",
    "            \n",
    "            # Get from and to zones\n",
    "            from_zone = current_row['zone_id']\n",
    "            to_zone = next_row['zone_id']\n",
    "            time_bin = current_row['time_bin']\n",
    "            \n",
    "            # Count all transitions, even those between the same zone\n",
    "            transition_matrices[time_bin].loc[from_zone, to_zone] += 1\n",
    "    \n",
    "    return transition_matrices\n",
    "\n",
    "\n",
    "transition_matrices_per_day = []\n",
    "\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Loop through each file and process it individually\n",
    "for file_path in parquet_files:\n",
    "    print(f\"\\nProcessing {file_path.name}...\")\n",
    "    \n",
    "    # Load the current parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Print information about this file\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Sample data:\")\n",
    "    print(df.head())\n",
    "    # Add timer before function call\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform any necessary operations on df here\n",
    "    #transition_matrices_per_day.append(create_zone_transition_matrix_approach1(df))\n",
    "    #df = create_zone_transitions_sequential_approach1(df)\n",
    "    \n",
    "    df = create_zone_transitions_sequential_approach2(df)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken to add to the df FROM/TO: {elapsed_time:.2f} seconds\")\n",
    "    calculate_nan_percentages(df)\n",
    "    \n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #transition_map = create_transition_hashmap(df)\n",
    "    #elapsed_time = time.time() - start_time\n",
    "    #print(f\"Processing transition matrix took: {elapsed_time:.2f} seconds\")\n",
    "    out_path = OUT_DIR / file_path.name\n",
    "\n",
    "    save_transitions_from_df(df, out_path)\n",
    "    # The dataframe will be garbage collected after each iteration\n",
    "    # as it goes out of scope\n",
    "    print(f\"Finished processing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
