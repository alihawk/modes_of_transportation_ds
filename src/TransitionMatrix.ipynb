{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Parquet files in data_binned\n"
     ]
    }
   ],
   "source": [
    "IN_DIR = Path(\"data_binned\") \n",
    "OUT_DIR = Path(\"data_transitions\")\n",
    "parquet_files = list(IN_DIR.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(parquet_files)} Parquet files in {IN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 20230331.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (87260420, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501965/148749262.py:178: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add to the df FROM/TO: 382.21 seconds\n",
      "Finished processing 20230331.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230328.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (86532152, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501965/148749262.py:178: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add to the df FROM/TO: 405.75 seconds\n",
      "Finished processing 20230328.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230327.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (87981395, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501965/148749262.py:178: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add to the df FROM/TO: 457.47 seconds\n",
      "Finished processing 20230327.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230401.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (78501155, 12)\n",
      "Time taken to add to the df FROM/TO: 375.11 seconds\n",
      "Finished processing 20230401.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230329.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (88523283, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501965/148749262.py:178: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add to the df FROM/TO: 499.47 seconds\n",
      "Finished processing 20230329.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230330.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (88407616, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1501965/148749262.py:178: UserWarning: Parsing dates in %d.%m.%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to add to the df FROM/TO: 474.47 seconds\n",
      "Finished processing 20230330.parquet\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing 20230402.parquet...\n",
      "Columns: ['deviceid', 'date', 'time', 'lon', 'lat', 'datetime', 'device_change', 'dist_m', 'dt', 'speed_m_s', 'zone_id', 'time_bin']\n",
      "Shape: (75946916, 12)\n",
      "Time taken to add to the df FROM/TO: 316.52 seconds\n",
      "Finished processing 20230402.parquet\n",
      "--------------------------------------------------\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "def save_transitions_from_df(df, output_file):\n",
    "    \"\"\"\n",
    "    Save transition data directly from a dataframe with FROM, TO and VALID columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with FROM, TO and VALID columns\n",
    "        output_file: Path to save the file\n",
    "        format: 'parquet'\n",
    "        \n",
    "        how it looks like:\n",
    "        time_bin  FROM      TO  count\n",
    "0         0       19.0    19.0     68\n",
    "1         0       19.0   787.0      1\n",
    "2         0       19.0  1435.0      2\n",
    "...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to only rows with valid transitions\n",
    "    valid_df = df[df['VALID'] == True].copy()\n",
    "    \n",
    "    # Group by time_bin, FROM, TO and count\n",
    "    transitions_df = valid_df.groupby(['time_bin', 'FROM', 'TO']).size().reset_index(name='count')\n",
    "\n",
    "    transitions_df.to_parquet(output_file, compression='snappy')\n",
    "    # Print statistics\n",
    "    file_size = os.path.getsize(output_file) / (1024*1024)\n",
    "    \n",
    "    print(f\"Saved transitions to {output_file}\")\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "    print(f\"Total transitions saved: {len(transitions_df)}\")\n",
    "    print(f\"Unique time bins: {len(transitions_df['time_bin'].unique())}\")\n",
    "    print(f\"Unique FROM zones: {len(transitions_df['FROM'].unique())}\")\n",
    "    print(f\"Unique TO zones: {len(transitions_df['TO'].unique())}\")\n",
    "    \n",
    "    return transitions_df\n",
    "\n",
    "def create_transition_hashmap(df):\n",
    "    \"\"\"\n",
    "    Create a transition matrix as a nested dictionary (hash map) from the dataframe.\n",
    "    Structure will be: result[time_bin][from_zone][to_zone] = count\n",
    "    \"\"\"\n",
    "    # Initialize the transition hash map\n",
    "    # Structure: transition_map[time_bin][from_zone][to_zone] = count\n",
    "    transition_map = {}\n",
    "    \n",
    "    # Filter to include only valid transitions\n",
    "    valid_df = df[df['VALID'] == True].copy()\n",
    "    \n",
    "    # Print basic stats\n",
    "    total_rows = len(df)\n",
    "    valid_rows = len(valid_df)\n",
    "    print(f\"Total rows in dataframe: {total_rows}\")\n",
    "    print(f\"Valid transitions: {valid_rows} ({valid_rows/total_rows*100:.2f}% of total)\")\n",
    "    \n",
    "    # Process each valid row\n",
    "    for _, row in valid_df.iterrows():\n",
    "        from_zone = row['FROM']  # Using the FROM column\n",
    "        to_zone = row['TO']      # Using the TO column\n",
    "        time_bin = row['time_bin']\n",
    "        \n",
    "        # Initialize nested dictionaries if they don't exist\n",
    "        if time_bin not in transition_map:\n",
    "            transition_map[time_bin] = {}\n",
    "        \n",
    "        if from_zone not in transition_map[time_bin]:\n",
    "            transition_map[time_bin][from_zone] = {}\n",
    "        \n",
    "        # Increment the transition count\n",
    "        if to_zone in transition_map[time_bin][from_zone]:\n",
    "            transition_map[time_bin][from_zone][to_zone] += 1\n",
    "        else:\n",
    "            transition_map[time_bin][from_zone][to_zone] = 1\n",
    "    \n",
    "    # Print transition statistics by time bin\n",
    "    print(\"\\nTransitions by time bin:\")\n",
    "    for time_bin in sorted(transition_map.keys()):\n",
    "        time_bin_transitions = sum(sum(counts.values()) for counts in transition_map[time_bin].values())\n",
    "        print(f\"  Time bin {time_bin}: {time_bin_transitions} transitions ({time_bin_transitions/valid_rows*100:.2f}% of valid)\")\n",
    "    \n",
    "    return transition_map\n",
    "\n",
    "def print_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate and print the percentage of valid and invalid transitions\n",
    "    using the VALID column.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Count valid transitions (VALID == True)\n",
    "    valid_transitions = df['VALID'].sum()\n",
    "    valid_pct = (valid_transitions / total_rows) * 100\n",
    "\n",
    "    # Count invalid transitions (VALID == False)\n",
    "    invalid_transitions = total_rows - valid_transitions\n",
    "    invalid_pct = (invalid_transitions / total_rows) * 100\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Valid transitions: {valid_transitions} ({valid_pct:.2f}%)\")\n",
    "    print(f\"Invalid transitions: {invalid_transitions} ({invalid_pct:.2f}%)\")\n",
    "\n",
    "def create_zone_transitions_sequential_approach1(df):\n",
    "    # Extract arrays from dataframe\n",
    "    zone_ids = df[\"zone_id\"].to_numpy()\n",
    "    time_bins = df[\"time_bin\"].to_numpy()\n",
    "    dc = df[\"device_change\"].to_numpy()\n",
    "    \n",
    "    # Create arrays for the next row's values using roll\n",
    "    zone_next = np.roll(zone_ids, -1)\n",
    "    time_bin_next = np.roll(time_bins, -1)\n",
    "    \n",
    "    # Handle device changes - mark the last row of each device\n",
    "    last_row = np.roll(dc, -1)\n",
    "    last_row[-1] = True  # Last row of the entire dataframe\n",
    "    \n",
    "    # Valid transitions are when:\n",
    "    # 1. Not at a device change boundary\n",
    "    # 2. Time bins are the same\n",
    "    valid_idx = (~last_row) & (time_bins == time_bin_next)\n",
    "    \n",
    "    # Create FROM and TO columns (use NaN for invalid transitions)\n",
    "    from_zones = np.full(len(df), np.nan, dtype=float)\n",
    "    to_zones = np.full(len(df), np.nan, dtype=float)\n",
    "    \n",
    "    # Set values only for valid transitions\n",
    "    from_zones[valid_idx] = zone_ids[valid_idx]\n",
    "    to_zones[valid_idx] = zone_next[valid_idx]\n",
    "    \n",
    "    # Add columns to dataframe\n",
    "    df[\"FROM\"] = from_zones\n",
    "    df[\"TO\"] = to_zones\n",
    "    \n",
    "    # Add VALID column - TRUE when both FROM and TO are not NaN\n",
    "    # This is equivalent to valid_idx\n",
    "    df[\"VALID\"] = valid_idx\n",
    "    \n",
    "    # Add the same_zone column for convenience\n",
    "    df[\"same_zone\"] = (zone_ids == zone_next) & valid_idx\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_zone_transition_matrix_approach1(df):\n",
    "    # Get unique zone_ids and time_bins\n",
    "    unique_zones = df['zone_id'].unique()\n",
    "    time_bins = df['time_bin'].unique()\n",
    "    \n",
    "    # Initialize transition matrices for each time_bin\n",
    "    transition_matrices = {tb: pd.DataFrame(0, index=unique_zones, columns=unique_zones) \n",
    "                          for tb in time_bins}\n",
    "    \n",
    "    # Iterate through rows sequentially\n",
    "    for i in range(len(df) - 1):\n",
    "        current_row = df.iloc[i]\n",
    "        next_row = df.iloc[i + 1]\n",
    "        \n",
    "        # Check if we're still tracking the same device and in the same time bin\n",
    "        if (not next_row['device_change'] and \n",
    "            current_row['time_bin'] == next_row['time_bin']):\n",
    "            \n",
    "            # Get from and to zones\n",
    "            from_zone = current_row['zone_id']\n",
    "            to_zone = next_row['zone_id']\n",
    "            time_bin = current_row['time_bin']\n",
    "            \n",
    "            # Count all transitions, even those between the same zone\n",
    "            transition_matrices[time_bin].loc[from_zone, to_zone] += 1\n",
    "    \n",
    "    return transition_matrices\n",
    "\n",
    "def create_zone_transition_by_dwell_time(df):\n",
    "    \"\"\"\n",
    "    Create a transition matrix based on dominant dwell zones per user and time_bin.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: FROM, TO, count\n",
    "    \"\"\"\n",
    "    # Ensure the timestamps are in datetime format\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"time\"].astype(str))\n",
    "\n",
    "    # Sort the dataframe for accurate duration calculation\n",
    "    df = df.sort_values(by=[\"deviceid\", \"time_bin\", \"timestamp\"])\n",
    "\n",
    "    # Calculate duration between consecutive points\n",
    "    df[\"next_timestamp\"] = df.groupby([\"deviceid\", \"time_bin\"])[\"timestamp\"].shift(-1)\n",
    "    df[\"duration_minutes\"] = (df[\"next_timestamp\"] - df[\"timestamp\"]).dt.total_seconds() / 60.0\n",
    "\n",
    "    # Assume final point in each group lasts 0 minute if not known\n",
    "    df[\"duration_minutes\"] = df[\"duration_minutes\"].fillna(0)\n",
    "\n",
    "    # Get zone with max dwell time per device per time_bin\n",
    "    dwell_df = df.groupby([\"deviceid\", \"time_bin\", \"zone_id\"])[\"duration_minutes\"].sum().reset_index()\n",
    "    dominant = dwell_df.loc[dwell_df.groupby([\"deviceid\", \"time_bin\"])[\"duration_minutes\"].idxmax()]\n",
    "\n",
    "    # Sort by device and time bin to prepare for transition extraction\n",
    "    dominant = dominant.sort_values(by=[\"deviceid\", \"time_bin\"])\n",
    "\n",
    "    # Shift to get next row's zone and time_bin\n",
    "    dominant[\"next_zone\"] = dominant.groupby(\"deviceid\")[\"zone_id\"].shift(-1)\n",
    "    dominant[\"next_time_bin\"] = dominant.groupby(\"deviceid\")[\"time_bin\"].shift(-1)\n",
    "\n",
    "    # Only keep transitions between consecutive time bins\n",
    "    valid_transitions = dominant[dominant[\"time_bin\"] + 1 == dominant[\"next_time_bin\"]].copy()\n",
    "\n",
    "    # Rename columns to match other transition formats\n",
    "    valid_transitions.rename(columns={\"zone_id\": \"FROM\", \"next_zone\": \"TO\"}, inplace=True)\n",
    "\n",
    "    # Count transitions\n",
    "    transition_matrix = (\n",
    "        valid_transitions.groupby([\"time_bin\", \"FROM\", \"TO\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "transition_matrices_per_day = []\n",
    "\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Loop through each file and process it individually\n",
    "for file_path in parquet_files:\n",
    "    print(f\"\\nProcessing {file_path.name}...\")\n",
    "    \n",
    "    # Load the current parquet file\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Print information about this file\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Add timer before function call\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform any necessary operations on df here\n",
    "    #transition_matrices_per_day.append(create_zone_transition_matrix_approach1(df))\n",
    "    df = create_zone_transition_by_dwell_time(df)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken to add to the df FROM/TO: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    #start_time = time.time()\n",
    "    #transition_map = create_transition_hashmap(df)\n",
    "    #elapsed_time = time.time() - start_time\n",
    "    #print(f\"Processing transition matrix took: {elapsed_time:.2f} seconds\")\n",
    "    out_path = OUT_DIR / file_path.name\n",
    "    \n",
    "    df.to_parquet(out_path, compression='snappy')\n",
    "    # The dataframe will be garbage collected after each iteration\n",
    "    # as it goes out of scope\n",
    "    print(f\"Finished processing {file_path.name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
