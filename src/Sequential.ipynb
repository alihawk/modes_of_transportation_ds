{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a3d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/jo83525/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2025-05-26 19:16:32 INFO: Loaded 54056 towers\n",
      "2025-05-26 19:16:32 INFO: === Processing 20230331.parquet ===\n",
      "2025-05-26 19:17:06 INFO: Original rows: 134,586,862\n",
      "2025-05-26 19:18:19 INFO: After tower-proximity denoise: 2,988,806 dropped (2.22%)\n",
      "2025-05-26 19:18:33 INFO: After repeated-coords removal: 12,091,544 dropped (8.98%)\n",
      "2025-05-26 19:18:34 INFO: Computing deltas\n",
      "2025-05-26 19:20:10 INFO: Zhang denoise\n",
      "2025-05-26 19:20:54 INFO: After Zhang denoise: 24,502,000 dropped (18.21%)\n",
      "2025-05-26 19:20:54 INFO: Sliding-window denoise\n",
      "2025-05-26 19:21:45 INFO: After sliding-window denoise: 231,508 dropped (0.17%)\n",
      "2025-05-26 19:22:58 INFO: Wrote data_denoised/20230331.parquet\n",
      "2025-05-26 19:22:58 INFO: Total dropped: 39,813,858 of 134,586,862 (29.58% removed)\n",
      "2025-05-26 19:22:58 INFO: === Processing 20230328.parquet ===\n",
      "2025-05-26 19:23:40 INFO: Original rows: 130,654,297\n",
      "2025-05-26 19:24:47 INFO: After tower-proximity denoise: 2,844,364 dropped (2.18%)\n",
      "2025-05-26 19:25:01 INFO: After repeated-coords removal: 10,531,860 dropped (8.06%)\n",
      "2025-05-26 19:25:02 INFO: Computing deltas\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# ───────────────────────── config ─────────────────────────\n",
    "SRC_DIR   = Path(\"data\")            # raw Parquets\n",
    "DST_DIR   = Path(\"data_denoised\")   # denoised output\n",
    "SKIP      = {\"slovenia_towers.parquet\"}\n",
    "\n",
    "R         = 6_371_000.0             # Earth radius (m)\n",
    "COUNT_THRESH = 150000               # coord-repeat threshold\n",
    "TIME_THRESH_S = 5*60                # long-stop threshold (seconds)\n",
    "TOWER_RADIUS_M = 10.0               # proximity for celltower_denoise\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Load cell‐tower list once\n",
    "df_towers = pd.read_parquet(\"data/slovenia_towers.parquet\")\n",
    "logging.info(\"Loaded %d towers\", len(df_towers))\n",
    "\n",
    "\n",
    "# ───────────── helpers: denoise ─────────────────\n",
    "\n",
    "def celltower_denoise(df: pd.DataFrame,\n",
    "                      towers: pd.DataFrame,\n",
    "                      radius_m: float = TOWER_RADIUS_M,\n",
    "                      earth_r: float = R) -> pd.DataFrame:\n",
    "    \"\"\"Drop any ping within `radius_m` of its nearest tower.\"\"\"\n",
    "    tower_rad = np.radians(towers[[\"LAT\",\"LON\"]].astype(\"float64\").values)\n",
    "    tree      = cKDTree(tower_rad)\n",
    "    pts_rad   = np.radians(df[[\"lat\",\"lon\"]].astype(\"float64\").values)\n",
    "    dist_rad, _ = tree.query(pts_rad, k=1)\n",
    "    dist_m      = dist_rad * earth_r\n",
    "\n",
    "    mask   = dist_m <= radius_m\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_repeated_coords(df: pd.DataFrame,\n",
    "                           count_thresh: int = COUNT_THRESH,\n",
    "                           squash: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove or squash (lat, lon) coordinates that occur >= count_thresh times globally.\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "\n",
    "    # Count global frequency of (lat, lon) pairs\n",
    "    coord_counts = df.groupby([\"lat\", \"lon\"]).size()\n",
    "    frequent_coords = coord_counts[coord_counts >= count_thresh].index\n",
    "\n",
    "    # Mark rows to drop\n",
    "    df[\"is_fallback\"] = df.set_index([\"lat\", \"lon\"]).index.isin(frequent_coords)\n",
    "\n",
    "    if squash:\n",
    "        df = df.sort_values([\"deviceid\", \"datetime\"])\n",
    "        df = df[~df.duplicated([\"lat\", \"lon\"], keep=\"first\")].copy()\n",
    "    else:\n",
    "        df = df[~df[\"is_fallback\"]].copy()\n",
    "\n",
    "    df.drop(columns=[\"is_fallback\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_long_stops(df: pd.DataFrame,\n",
    "                      time_thresh_s: float = TIME_THRESH_S,\n",
    "                      squash: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop or squash runs of identical coords PER DEVICE whose total dwell ≥ time_thresh_s.\n",
    "    Assumes df is sorted by deviceid/datetime and has df['dt'] (seconds).\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "    df = df.sort_values(['deviceid','datetime']).reset_index(drop=True)\n",
    "\n",
    "    same = (\n",
    "        (df['deviceid'] == df['deviceid'].shift(1)) &\n",
    "        (df['lat']       == df['lat'].shift(1)) &\n",
    "        (df['lon']       == df['lon'].shift(1))\n",
    "    )\n",
    "    df['is_repeat'] = same\n",
    "    df['run_id']    = (~df['is_repeat']).cumsum()\n",
    "\n",
    "    run_durs = (\n",
    "        df.groupby(['deviceid','run_id'])['dt']\n",
    "          .sum()\n",
    "          .reset_index(name='run_dur_s')\n",
    "    )\n",
    "    df = df.merge(run_durs, on=['deviceid','run_id'], how='left')\n",
    "\n",
    "    if squash:\n",
    "        mask = ~((df['is_repeat']) & (df['run_dur_s'] >= time_thresh_s))\n",
    "        df = df[mask]\n",
    "        df = df.drop_duplicates(['deviceid','run_id'], keep='first')\n",
    "    else:\n",
    "        df = df[~((df['is_repeat']) & (df['run_dur_s'] >= time_thresh_s))]\n",
    "\n",
    "    df.drop(columns=['is_repeat','run_id','run_dur_s'], inplace=True)\n",
    "    dropped = before - len(df)\n",
    "    logging.info(\"remove_long_stops: dropped %d of %d pings (%.2f%%) in runs ≥%ds\",\n",
    "                 dropped, before, 100*dropped/before if before else 0.0, time_thresh_s)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ───────────── helpers: existing pipeline ─────────────────\n",
    "\n",
    "def sequential_deltas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add device_change, dist_m, dt, speed_m_s (in-place).\"\"\"\n",
    "    df[\"date\"]     = df[\"date\"].astype(str)\n",
    "    df[\"time\"]     = df[\"time\"].astype(str)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time\"],\n",
    "                                    dayfirst=True)\n",
    "\n",
    "    df[\"deviceid\"] = df[\"deviceid\"].astype(\"category\")\n",
    "    df.sort_values([\"deviceid\", \"datetime\"], inplace=True, ignore_index=True)\n",
    "\n",
    "    dc = (df[\"deviceid\"] != df[\"deviceid\"].shift()).to_numpy()\n",
    "    df[\"device_change\"] = dc\n",
    "\n",
    "    lat = np.radians(df[\"lat\"].to_numpy())\n",
    "    lon = np.radians(df[\"lon\"].to_numpy())\n",
    "    t   = (df[\"datetime\"].astype(\"int64\").to_numpy() //\n",
    "           1_000_000_000)\n",
    "\n",
    "    lat_prev = np.roll(lat, 1); lon_prev = np.roll(lon, 1); t_prev = np.roll(t, 1)\n",
    "    lat_prev[dc] = lat[dc]; lon_prev[dc] = lon[dc]; t_prev[dc] = t[dc]\n",
    "\n",
    "    dlat = lat - lat_prev\n",
    "    dlon = lon - lon_prev\n",
    "    a    = np.sin(dlat/2)**2 + np.cos(lat)*np.cos(lat_prev)*np.sin(dlon/2)**2\n",
    "    dist = R * (2*np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    dt   = (t - t_prev).clip(min=1)\n",
    "\n",
    "    dist[dc] = 0.0; dt[dc] = 0.0\n",
    "    speed    = np.divide(dist, dt, out=np.zeros_like(dist), where=dt > 0)\n",
    "\n",
    "    df[\"dist_m\"]    = dist\n",
    "    df[\"dt\"]        = dt\n",
    "    df[\"speed_m_s\"] = speed\n",
    "    df[\"lat_rad\"]   = lat          # for angle later\n",
    "    df[\"lon_rad\"]   = lon\n",
    "    return df\n",
    "\n",
    "\n",
    "def _bearing(lat1, lon1, lat2, lon2):\n",
    "    dλ = lon2 - lon1\n",
    "    x  = np.sin(dλ)*np.cos(lat2)\n",
    "    y  = np.cos(lat1)*np.sin(lat2) - np.sin(lat1)*np.cos(lat2)*np.cos(dλ)\n",
    "    return (np.degrees(np.arctan2(x,y)) + 360.0) % 360.0\n",
    "\n",
    "\n",
    "def zhang_denoise(df, speed_th=30, angle_th=30, time_th=10):\n",
    "    lat = df['lat_rad'].to_numpy(); lon = df['lon_rad'].to_numpy()\n",
    "    t   = (df['datetime'].astype('int64')//1_000_000_000).astype(float)\n",
    "    spd = df['speed_m_s'].to_numpy(); dc = df['device_change'].to_numpy()\n",
    "\n",
    "    lat_prev = np.roll(lat,1); lon_prev = np.roll(lon,1); t_prev = np.roll(t,1)\n",
    "    lat_next = np.roll(lat,-1); lon_next = np.roll(lon,-1)\n",
    "\n",
    "    lat_prev[dc] = lon_prev[dc] = t_prev[dc] = np.nan\n",
    "    last = np.roll(dc,-1); last[-1]=True\n",
    "    lat_next[last] = lon_next[last] = np.nan\n",
    "\n",
    "    b1  = _bearing(lat_prev, lon_prev, lat, lon)\n",
    "    b2  = _bearing(lat, lon, lat_next, lon_next)\n",
    "    ang = np.abs(b2 - b1); ang = np.where(ang>180,360-ang,ang)\n",
    "\n",
    "    dt  = t - t_prev\n",
    "    keep = (spd < speed_th) & ((ang > angle_th)|(dt > time_th))\n",
    "    keep &= ~np.isnan(lat_prev) & ~np.isnan(lat_next)\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def sliding_window_denoise(df, window=5, speed_th=40, min_pts=3):\n",
    "    spd = df['speed_m_s'].to_numpy(); dc = df['device_change'].to_numpy()\n",
    "    keep = np.ones(len(df), dtype=bool)\n",
    "    start = 0\n",
    "    for i in range(1,len(df)+1):\n",
    "        if i==len(df) or dc[i]:\n",
    "            med = pd.Series(spd[start:i]).rolling(window,center=True,min_periods=1).median().to_numpy()\n",
    "            keep[start:i] &= med < speed_th\n",
    "            start = i\n",
    "    valid = df.loc[keep,'deviceid'].value_counts().loc[lambda s:s>min_pts].index\n",
    "    keep &= df['deviceid'].isin(valid)\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ─────────────────────── main loop ───────────────────────\n",
    "\n",
    "def main():\n",
    "    DST_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for fp in SRC_DIR.glob(\"*.parquet\"):\n",
    "        if fp.name in SKIP:\n",
    "            logging.info(f\"Skipping {fp.name}\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"=== Processing {fp.name} ===\")\n",
    "        df = pd.read_parquet(fp)\n",
    "        orig = len(df)\n",
    "        prev_len = orig\n",
    "        logging.info(f\"Original rows: {orig:,}\")\n",
    "\n",
    "        df[['lat','lon']] = df[['lat','lon']].astype('float32')\n",
    "\n",
    "        # 1) Tower-proximity denoise\n",
    "        df = celltower_denoise(df, df_towers, radius_m=TOWER_RADIUS_M)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After tower-proximity denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 2) Remove repeated coordinates\n",
    "        df = remove_repeated_coords(df, count_thresh=COUNT_THRESH, squash=False)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After repeated-coords removal: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 3) Encode deviceid and compute deltas\n",
    "        codes, _ = pd.factorize(df['deviceid'], sort=False)\n",
    "        df['deviceid'] = codes.astype('int32')\n",
    "\n",
    "        logging.info(\"Computing deltas\")\n",
    "        df = sequential_deltas(df)\n",
    "\n",
    "        # 4) Zhang denoise\n",
    "        logging.info(\"Zhang denoise\")\n",
    "        df = zhang_denoise(df)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After Zhang denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 5) Sliding-window denoise\n",
    "        logging.info(\"Sliding-window denoise\")\n",
    "        df = sliding_window_denoise(df)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After sliding-window denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # Cleanup and save\n",
    "        df.drop(columns=[\"lat_rad\", \"lon_rad\"], inplace=True)\n",
    "        out_path = DST_DIR / fp.name\n",
    "        df.to_parquet(out_path, index=False, compression=\"snappy\")\n",
    "        logging.info(f\"Wrote {out_path}\")\n",
    "\n",
    "        # Final summary\n",
    "        final_rows = len(df)\n",
    "        total_dropped = orig - final_rows\n",
    "        logging.info(f\"Total dropped: {total_dropped:,} of {orig:,} ({100 * total_dropped / orig:.2f}% removed)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
