{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2a3d66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/jo83525/.local/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "2025-05-30 11:06:20 INFO: Loaded 54056 towers\n",
      "2025-05-30 11:06:20 INFO: === Processing 20230331.parquet ===\n",
      "2025-05-30 11:07:13 INFO: Original rows: 134,586,862\n",
      "2025-05-30 11:07:13 INFO: Original devices: 802,897\n",
      "2025-05-30 11:10:22 INFO: After tower-proximity denoise: 2,988,806 dropped (2.22%)\n",
      "2025-05-30 11:10:23 INFO: Devices: 798,712 (99.48%)\n",
      "2025-05-30 11:10:49 INFO: After repeated-coords removal: 12,091,544 dropped (8.98%)\n",
      "2025-05-30 11:10:50 INFO: Devices: 786,487 (97.96%)\n",
      "2025-05-30 11:10:52 INFO: Computing deltas\n",
      "2025-05-30 11:14:45 INFO: Zheng denoise\n",
      "2025-05-30 11:18:31 INFO: After Zheng denoise: 24,502,000 dropped (18.21%)\n",
      "2025-05-30 11:18:32 INFO: Devices: 675,881 (84.18%)\n",
      "2025-05-30 11:18:32 INFO: Sliding-window denoise\n",
      "2025-05-30 11:21:26 INFO: After sliding-window denoise: 7,503,731 dropped (5.58%)\n",
      "2025-05-30 11:21:27 INFO: Devices: 670,327 (83.49%)\n",
      "2025-05-30 11:22:44 INFO: After min-points-per-device filter: 240,328 dropped (0.18%)\n",
      "2025-05-30 11:22:46 INFO: Devices: 542,576 (67.58%)\n",
      "2025-05-30 11:24:30 INFO: Wrote data_denoised/20230331.parquet\n",
      "2025-05-30 11:24:30 INFO: Total dropped: 47,326,409 of 134,586,862 (35.16% removed)\n",
      "2025-05-30 11:24:30 INFO: === Processing 20230328.parquet ===\n",
      "2025-05-30 11:25:44 INFO: Original rows: 130,654,297\n",
      "2025-05-30 11:25:44 INFO: Original devices: 749,459\n",
      "2025-05-30 11:28:52 INFO: After tower-proximity denoise: 2,844,364 dropped (2.18%)\n",
      "2025-05-30 11:28:55 INFO: Devices: 745,311 (99.45%)\n",
      "2025-05-30 11:29:39 INFO: After repeated-coords removal: 10,531,860 dropped (8.06%)\n",
      "2025-05-30 11:29:41 INFO: Devices: 733,819 (97.91%)\n",
      "2025-05-30 11:29:43 INFO: Computing deltas\n",
      "2025-05-30 11:33:15 INFO: Zheng denoise\n",
      "2025-05-30 11:37:10 INFO: After Zheng denoise: 23,377,617 dropped (17.89%)\n",
      "2025-05-30 11:37:11 INFO: Devices: 620,152 (82.75%)\n",
      "2025-05-30 11:37:11 INFO: Sliding-window denoise\n",
      "2025-05-30 11:38:57 INFO: After sliding-window denoise: 7,149,494 dropped (5.47%)\n",
      "2025-05-30 11:38:58 INFO: Devices: 616,144 (82.21%)\n",
      "2025-05-30 11:39:46 INFO: After min-points-per-device filter: 218,753 dropped (0.17%)\n",
      "2025-05-30 11:39:47 INFO: Devices: 500,313 (66.76%)\n",
      "2025-05-30 11:41:29 INFO: Wrote data_denoised/20230328.parquet\n",
      "2025-05-30 11:41:29 INFO: Total dropped: 44,122,088 of 130,654,297 (33.77% removed)\n",
      "2025-05-30 11:41:29 INFO: === Processing 20230327.parquet ===\n",
      "2025-05-30 11:42:43 INFO: Original rows: 133,453,155\n",
      "2025-05-30 11:42:43 INFO: Original devices: 751,168\n",
      "2025-05-30 11:45:02 INFO: After tower-proximity denoise: 2,847,141 dropped (2.13%)\n",
      "2025-05-30 11:45:03 INFO: Devices: 747,158 (99.47%)\n",
      "2025-05-30 11:45:30 INFO: After repeated-coords removal: 12,118,981 dropped (9.08%)\n",
      "2025-05-30 11:45:31 INFO: Devices: 734,334 (97.76%)\n",
      "2025-05-30 11:45:33 INFO: Computing deltas\n",
      "2025-05-30 11:48:41 INFO: Zheng denoise\n",
      "2025-05-30 11:51:13 INFO: After Zheng denoise: 23,170,170 dropped (17.36%)\n",
      "2025-05-30 11:51:13 INFO: Devices: 621,497 (82.74%)\n",
      "2025-05-30 11:51:13 INFO: Sliding-window denoise\n",
      "2025-05-30 11:52:57 INFO: After sliding-window denoise: 7,116,032 dropped (5.33%)\n",
      "2025-05-30 11:52:58 INFO: Devices: 617,225 (82.17%)\n",
      "2025-05-30 11:53:44 INFO: After min-points-per-device filter: 219,412 dropped (0.16%)\n",
      "2025-05-30 11:53:45 INFO: Devices: 501,715 (66.79%)\n",
      "2025-05-30 11:55:27 INFO: Wrote data_denoised/20230327.parquet\n",
      "2025-05-30 11:55:27 INFO: Total dropped: 45,471,736 of 133,453,155 (34.07% removed)\n",
      "2025-05-30 11:55:27 INFO: === Processing 20230401.parquet ===\n",
      "2025-05-30 11:56:35 INFO: Original rows: 117,190,239\n",
      "2025-05-30 11:56:35 INFO: Original devices: 751,629\n",
      "2025-05-30 11:58:34 INFO: After tower-proximity denoise: 2,774,762 dropped (2.37%)\n",
      "2025-05-30 11:58:36 INFO: Devices: 747,131 (99.40%)\n",
      "2025-05-30 11:58:57 INFO: After repeated-coords removal: 8,067,063 dropped (6.88%)\n",
      "2025-05-30 11:58:59 INFO: Devices: 737,831 (98.16%)\n",
      "2025-05-30 11:59:00 INFO: Computing deltas\n",
      "2025-05-30 12:01:39 INFO: Zheng denoise\n",
      "2025-05-30 12:04:00 INFO: After Zheng denoise: 21,411,250 dropped (18.27%)\n",
      "2025-05-30 12:04:01 INFO: Devices: 629,607 (83.77%)\n",
      "2025-05-30 12:04:01 INFO: Sliding-window denoise\n",
      "2025-05-30 12:05:40 INFO: After sliding-window denoise: 6,196,631 dropped (5.29%)\n",
      "2025-05-30 12:05:41 INFO: Devices: 624,043 (83.03%)\n",
      "2025-05-30 12:06:33 INFO: After min-points-per-device filter: 239,364 dropped (0.20%)\n",
      "2025-05-30 12:06:34 INFO: Devices: 497,995 (66.26%)\n",
      "2025-05-30 12:08:12 INFO: Wrote data_denoised/20230401.parquet\n",
      "2025-05-30 12:08:12 INFO: Total dropped: 38,689,070 of 117,190,239 (33.01% removed)\n",
      "2025-05-30 12:08:12 INFO: Skipping slovenia_towers.parquet\n",
      "2025-05-30 12:08:12 INFO: === Processing 20230329.parquet ===\n",
      "2025-05-30 12:09:26 INFO: Original rows: 135,320,914\n",
      "2025-05-30 12:09:26 INFO: Original devices: 758,892\n",
      "2025-05-30 12:11:42 INFO: After tower-proximity denoise: 2,946,316 dropped (2.18%)\n",
      "2025-05-30 12:11:44 INFO: Devices: 754,894 (99.47%)\n",
      "2025-05-30 12:12:08 INFO: After repeated-coords removal: 12,238,073 dropped (9.04%)\n",
      "2025-05-30 12:12:10 INFO: Devices: 741,925 (97.76%)\n",
      "2025-05-30 12:12:12 INFO: Computing deltas\n",
      "2025-05-30 12:15:09 INFO: Zheng denoise\n",
      "2025-05-30 12:17:57 INFO: After Zheng denoise: 23,979,851 dropped (17.72%)\n",
      "2025-05-30 12:17:58 INFO: Devices: 629,536 (82.95%)\n",
      "2025-05-30 12:17:58 INFO: Sliding-window denoise\n",
      "2025-05-30 12:19:52 INFO: After sliding-window denoise: 7,409,292 dropped (5.48%)\n",
      "2025-05-30 12:19:53 INFO: Devices: 625,513 (82.42%)\n",
      "2025-05-30 12:20:46 INFO: After min-points-per-device filter: 224,079 dropped (0.17%)\n",
      "2025-05-30 12:20:46 INFO: Devices: 507,342 (66.85%)\n",
      "2025-05-30 12:22:30 INFO: Wrote data_denoised/20230329.parquet\n",
      "2025-05-30 12:22:30 INFO: Total dropped: 46,797,611 of 135,320,914 (34.58% removed)\n",
      "2025-05-30 12:22:30 INFO: === Processing 20230330.parquet ===\n",
      "2025-05-30 12:23:44 INFO: Original rows: 135,827,994\n",
      "2025-05-30 12:23:44 INFO: Original devices: 768,525\n",
      "2025-05-30 12:26:19 INFO: After tower-proximity denoise: 3,010,719 dropped (2.22%)\n",
      "2025-05-30 12:26:20 INFO: Devices: 764,313 (99.45%)\n",
      "2025-05-30 12:26:47 INFO: After repeated-coords removal: 12,505,550 dropped (9.21%)\n",
      "2025-05-30 12:26:48 INFO: Devices: 750,525 (97.66%)\n",
      "2025-05-30 12:26:51 INFO: Computing deltas\n",
      "2025-05-30 12:30:01 INFO: Zheng denoise\n",
      "2025-05-30 12:33:05 INFO: After Zheng denoise: 24,240,697 dropped (17.85%)\n",
      "2025-05-30 12:33:06 INFO: Devices: 638,727 (83.11%)\n",
      "2025-05-30 12:33:06 INFO: Sliding-window denoise\n",
      "2025-05-30 12:34:54 INFO: After sliding-window denoise: 7,435,726 dropped (5.47%)\n",
      "2025-05-30 12:34:55 INFO: Devices: 634,217 (82.52%)\n",
      "2025-05-30 12:35:44 INFO: After min-points-per-device filter: 227,651 dropped (0.17%)\n",
      "2025-05-30 12:35:45 INFO: Devices: 513,656 (66.84%)\n",
      "2025-05-30 12:37:32 INFO: Wrote data_denoised/20230330.parquet\n",
      "2025-05-30 12:37:32 INFO: Total dropped: 47,420,343 of 135,827,994 (34.91% removed)\n",
      "2025-05-30 12:37:32 INFO: === Processing 20230402.parquet ===\n",
      "2025-05-30 12:38:27 INFO: Original rows: 110,660,627\n",
      "2025-05-30 12:38:27 INFO: Original devices: 693,142\n",
      "2025-05-30 12:40:11 INFO: After tower-proximity denoise: 2,063,257 dropped (1.86%)\n",
      "2025-05-30 12:40:12 INFO: Devices: 689,171 (99.43%)\n",
      "2025-05-30 12:40:29 INFO: After repeated-coords removal: 7,103,078 dropped (6.42%)\n",
      "2025-05-30 12:40:30 INFO: Devices: 680,187 (98.13%)\n",
      "2025-05-30 12:40:32 INFO: Computing deltas\n",
      "2025-05-30 12:42:38 INFO: Zheng denoise\n",
      "2025-05-30 12:44:14 INFO: After Zheng denoise: 19,685,357 dropped (17.79%)\n",
      "2025-05-30 12:44:15 INFO: Devices: 568,454 (82.01%)\n",
      "2025-05-30 12:44:15 INFO: Sliding-window denoise\n",
      "2025-05-30 12:45:34 INFO: After sliding-window denoise: 5,636,563 dropped (5.09%)\n",
      "2025-05-30 12:45:35 INFO: Devices: 563,785 (81.34%)\n",
      "2025-05-30 12:46:11 INFO: After min-points-per-device filter: 225,429 dropped (0.20%)\n",
      "2025-05-30 12:46:12 INFO: Devices: 448,129 (64.65%)\n",
      "2025-05-30 12:47:31 INFO: Wrote data_denoised/20230402.parquet\n",
      "2025-05-30 12:47:31 INFO: Total dropped: 34,713,684 of 110,660,627 (31.37% removed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# ───────────────────────── config ─────────────────────────\n",
    "SRC_DIR   = Path(\"data\")            # raw Parquets\n",
    "DST_DIR   = Path(\"data_denoised\")   # denoised output\n",
    "SKIP      = {\"slovenia_towers.parquet\"}\n",
    "\n",
    "R         = 6_371_000.0             # Earth radius (m)\n",
    "COUNT_THRESH = 150000               # coord-repeat threshold\n",
    "TIME_THRESH_S = 5*60                # long-stop threshold (seconds)\n",
    "TOWER_RADIUS_M = 10.0               # proximity for celltower_denoise\n",
    "MIN_POINTS_PER_DEVICE = 3           # min points-per-device\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "# Load cell‐tower list once\n",
    "df_towers = pd.read_parquet(\"data/slovenia_towers.parquet\")\n",
    "logging.info(\"Loaded %d towers\", len(df_towers))\n",
    "\n",
    "\n",
    "# ───────────── helpers: denoise ─────────────────\n",
    "\n",
    "def celltower_denoise(df: pd.DataFrame,\n",
    "                      towers: pd.DataFrame,\n",
    "                      radius_m: float = TOWER_RADIUS_M,\n",
    "                      earth_r: float = R) -> pd.DataFrame:\n",
    "    \"\"\"Drop any ping within `radius_m` of its nearest tower.\"\"\"\n",
    "    tower_rad = np.radians(towers[[\"LAT\",\"LON\"]].astype(\"float64\").values)\n",
    "    tree      = cKDTree(tower_rad)\n",
    "    pts_rad   = np.radians(df[[\"lat\",\"lon\"]].astype(\"float64\").values)\n",
    "    dist_rad, _ = tree.query(pts_rad, k=1)\n",
    "    dist_m      = dist_rad * earth_r\n",
    "\n",
    "    mask   = dist_m <= radius_m\n",
    "    df = df[~mask].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_repeated_coords(df: pd.DataFrame,\n",
    "                           count_thresh: int = COUNT_THRESH,\n",
    "                           squash: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove or squash (lat, lon) coordinates that occur >= count_thresh times globally.\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "\n",
    "    # Count global frequency of (lat, lon) pairs\n",
    "    coord_counts = df.groupby([\"lat\", \"lon\"]).size()\n",
    "    frequent_coords = coord_counts[coord_counts >= count_thresh].index\n",
    "\n",
    "    # Mark rows to drop\n",
    "    df[\"is_fallback\"] = df.set_index([\"lat\", \"lon\"]).index.isin(frequent_coords)\n",
    "\n",
    "    if squash:\n",
    "        df = df.sort_values([\"deviceid\", \"datetime\"])\n",
    "        df = df[~df.duplicated([\"lat\", \"lon\"], keep=\"first\")].copy()\n",
    "    else:\n",
    "        df = df[~df[\"is_fallback\"]].copy()\n",
    "\n",
    "    df.drop(columns=[\"is_fallback\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_long_stops(df: pd.DataFrame,\n",
    "                      time_thresh_s: float = TIME_THRESH_S,\n",
    "                      squash: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop or squash runs of identical coords PER DEVICE whose total dwell ≥ time_thresh_s.\n",
    "    Assumes df is sorted by deviceid/datetime and has df['dt'] (seconds).\n",
    "    \"\"\"\n",
    "    before = len(df)\n",
    "    df = df.sort_values(['deviceid','datetime']).reset_index(drop=True)\n",
    "\n",
    "    same = (\n",
    "        (df['deviceid'] == df['deviceid'].shift(1)) &\n",
    "        (df['lat']       == df['lat'].shift(1)) &\n",
    "        (df['lon']       == df['lon'].shift(1))\n",
    "    )\n",
    "    df['is_repeat'] = same\n",
    "    df['run_id']    = (~df['is_repeat']).cumsum()\n",
    "\n",
    "    run_durs = (\n",
    "        df.groupby(['deviceid','run_id'])['dt']\n",
    "          .sum()\n",
    "          .reset_index(name='run_dur_s')\n",
    "    )\n",
    "    df = df.merge(run_durs, on=['deviceid','run_id'], how='left')\n",
    "\n",
    "    if squash:\n",
    "        mask = ~((df['is_repeat']) & (df['run_dur_s'] >= time_thresh_s))\n",
    "        df = df[mask]\n",
    "        df = df.drop_duplicates(['deviceid','run_id'], keep='first')\n",
    "    else:\n",
    "        df = df[~((df['is_repeat']) & (df['run_dur_s'] >= time_thresh_s))]\n",
    "\n",
    "    df.drop(columns=['is_repeat','run_id','run_dur_s'], inplace=True)\n",
    "    dropped = before - len(df)\n",
    "    logging.info(\"remove_long_stops: dropped %d of %d pings (%.2f%%) in runs ≥%ds\",\n",
    "                 dropped, before, 100*dropped/before if before else 0.0, time_thresh_s)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ───────────── helpers: existing pipeline ─────────────────\n",
    "\n",
    "def sequential_deltas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add device_change, dist_m, dt, speed_m_s (in-place).\"\"\"\n",
    "    df[\"date\"]     = df[\"date\"].astype(str)\n",
    "    df[\"time\"]     = df[\"time\"].astype(str)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"date\"] + \" \" + df[\"time\"],\n",
    "                                    dayfirst=True)\n",
    "\n",
    "    df[\"deviceid\"] = df[\"deviceid\"].astype(\"category\")\n",
    "    df.sort_values([\"deviceid\", \"datetime\"], inplace=True, ignore_index=True)\n",
    "\n",
    "    dc = (df[\"deviceid\"] != df[\"deviceid\"].shift()).to_numpy()\n",
    "    df[\"device_change\"] = dc\n",
    "\n",
    "    lat = np.radians(df[\"lat\"].to_numpy())\n",
    "    lon = np.radians(df[\"lon\"].to_numpy())\n",
    "    t   = (df[\"datetime\"].astype(\"int64\").to_numpy() //\n",
    "           1_000_000_000)\n",
    "\n",
    "    lat_prev = np.roll(lat, 1); lon_prev = np.roll(lon, 1); t_prev = np.roll(t, 1)\n",
    "    lat_prev[dc] = lat[dc]; lon_prev[dc] = lon[dc]; t_prev[dc] = t[dc]\n",
    "\n",
    "    dlat = lat - lat_prev\n",
    "    dlon = lon - lon_prev\n",
    "    a    = np.sin(dlat/2)**2 + np.cos(lat)*np.cos(lat_prev)*np.sin(dlon/2)**2\n",
    "    dist = R * (2*np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    dt   = (t - t_prev).clip(min=1)\n",
    "\n",
    "    dist[dc] = 0.0; dt[dc] = 0.0\n",
    "    speed    = np.divide(dist, dt, out=np.zeros_like(dist), where=dt > 0)\n",
    "\n",
    "    df[\"dist_m\"]    = dist\n",
    "    df[\"dt\"]        = dt\n",
    "    df[\"speed_m_s\"] = speed\n",
    "    df[\"lat_rad\"]   = lat          # for angle later\n",
    "    df[\"lon_rad\"]   = lon\n",
    "    return df\n",
    "\n",
    "\n",
    "def _bearing(lat1, lon1, lat2, lon2):\n",
    "    dλ = lon2 - lon1\n",
    "    x  = np.sin(dλ)*np.cos(lat2)\n",
    "    y  = np.cos(lat1)*np.sin(lat2) - np.sin(lat1)*np.cos(lat2)*np.cos(dλ)\n",
    "    return (np.degrees(np.arctan2(x,y)) + 360.0) % 360.0\n",
    "\n",
    "\n",
    "def zheng_denoise(df, speed_th=30, angle_th=30, time_th=10, earth_r=R):\n",
    "    \"\"\"\n",
    "    Fast, vectorized Zheng-like denoise that avoids groupby by using device_change mask.\n",
    "    It drops points that violate speed, angle, and temporal rules, and handles boundary edges properly.\n",
    "    \"\"\"\n",
    "\n",
    "    lat = df[\"lat_rad\"].to_numpy()\n",
    "    lon = df[\"lon_rad\"].to_numpy()\n",
    "    t   = (df[\"datetime\"].astype(\"int64\") // 1_000_000_000).astype(\"float64\")\n",
    "    dc  = df[\"device_change\"].to_numpy()\n",
    "\n",
    "    # Create shifted versions\n",
    "    lat_prev = np.roll(lat, 1)\n",
    "    lon_prev = np.roll(lon, 1)\n",
    "    t_prev   = np.roll(t, 1)\n",
    "\n",
    "    lat_next = np.roll(lat, -1)\n",
    "    lon_next = np.roll(lon, -1)\n",
    "\n",
    "    # Mask invalid neighbors (due to device change)\n",
    "    lat_prev[dc] = np.nan\n",
    "    lon_prev[dc] = np.nan\n",
    "    t_prev[dc]   = np.nan\n",
    "\n",
    "    last = np.roll(dc, -1)\n",
    "    last[-1] = True\n",
    "    lat_next[last] = np.nan\n",
    "    lon_next[last] = np.nan\n",
    "\n",
    "    # Haversine to previous point\n",
    "    dlat = lat - lat_prev\n",
    "    dlon = lon - lon_prev\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat) * np.cos(lat_prev) * np.sin(dlon / 2)**2\n",
    "    dist = 2 * earth_r * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    dt = t - t_prev\n",
    "    dt = np.clip(dt, 1, None)\n",
    "    speed = dist / dt\n",
    "\n",
    "    # Angle between prev -> current -> next\n",
    "    def bearing(lat1, lon1, lat2, lon2):\n",
    "        dlon = lon2 - lon1\n",
    "        x = np.sin(dlon) * np.cos(lat2)\n",
    "        y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "        return np.arctan2(x, y)\n",
    "\n",
    "    b1 = bearing(lat_prev, lon_prev, lat, lon)\n",
    "    b2 = bearing(lat, lon, lat_next, lon_next)\n",
    "    ang = np.abs(b2 - b1)\n",
    "    ang = np.where(ang > np.pi, 2 * np.pi - ang, ang)\n",
    "    ang = np.degrees(ang)\n",
    "\n",
    "    # Validity criteria\n",
    "    keep = (speed < speed_th) & ((ang > angle_th) | (dt > time_th))\n",
    "    keep &= ~np.isnan(lat_prev) & ~np.isnan(lat_next)\n",
    "\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def sliding_window_denoise(df, window=5, speed_th=40.0, margin=5.0):\n",
    "    \"\"\"\n",
    "    Remove points whose speed is significantly above the local median.\n",
    "    Uses a rolling median over each device's speed.\n",
    "    \n",
    "    Parameters:\n",
    "    - window: size of the rolling window\n",
    "    - speed_th: base speed threshold in m/s\n",
    "    - margin: tolerance above local median allowed\n",
    "\n",
    "    Returns:\n",
    "    - filtered DataFrame\n",
    "    \"\"\"\n",
    "    spd = df['speed_m_s'].to_numpy()\n",
    "    dc  = df['device_change'].to_numpy()\n",
    "    keep = np.ones(len(df), dtype=bool)\n",
    "    start = 0\n",
    "\n",
    "    for i in range(1, len(df) + 1):\n",
    "        if i == len(df) or dc[i]:\n",
    "            # Median over the segment\n",
    "            segment_spd = spd[start:i]\n",
    "            median = pd.Series(segment_spd).rolling(window, center=True, min_periods=1).median().to_numpy()\n",
    "\n",
    "            # Only keep points close to or under median + margin\n",
    "            keep[start:i] &= segment_spd <= (median + margin)\n",
    "\n",
    "            start = i\n",
    "\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ─────────────────────── main loop ───────────────────────\n",
    "\n",
    "def main():\n",
    "    DST_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for fp in SRC_DIR.glob(\"*.parquet\"):\n",
    "        if fp.name in SKIP:\n",
    "            logging.info(f\"Skipping {fp.name}\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"=== Processing {fp.name} ===\")\n",
    "        df = pd.read_parquet(fp)\n",
    "        orig = len(df)\n",
    "        orig_devices = df['deviceid'].nunique()\n",
    "        prev_len = orig\n",
    "        logging.info(f\"Original rows: {orig:,}\")\n",
    "        logging.info(f\"Original devices: {orig_devices:,}\")\n",
    "\n",
    "        df[['lat','lon']] = df[['lat','lon']].astype('float32')\n",
    "\n",
    "        # 1) Tower-proximity denoise\n",
    "        df = celltower_denoise(df, df_towers, radius_m=TOWER_RADIUS_M)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After tower-proximity denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        logging.info(f\"Devices: {df['deviceid'].nunique():,} ({100 * df['deviceid'].nunique() / orig_devices:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "\n",
    "        # 2) Remove repeated coordinates\n",
    "        df = remove_repeated_coords(df, count_thresh=COUNT_THRESH, squash=False)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After repeated-coords removal: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        logging.info(f\"Devices: {df['deviceid'].nunique():,} ({100 * df['deviceid'].nunique() / orig_devices:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 3) Encode deviceid and compute deltas\n",
    "        df[\"original_deviceid\"] = df[\"deviceid\"]  # Keep original ID for later\n",
    "        codes, _ = pd.factorize(df['deviceid'], sort=False)\n",
    "        df['deviceid'] = codes.astype('int32')\n",
    "\n",
    "        logging.info(\"Computing deltas\")\n",
    "        df = sequential_deltas(df)\n",
    "\n",
    "        # 4) Zheng denoise\n",
    "        logging.info(\"Zheng denoise\")\n",
    "        df = zheng_denoise(df)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After Zheng denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        logging.info(f\"Devices: {df['deviceid'].nunique():,} ({100 * df['deviceid'].nunique() / orig_devices:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 5) Sliding-window denoise\n",
    "        logging.info(\"Sliding-window denoise\")\n",
    "        df = sliding_window_denoise(df)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After sliding-window denoise: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        logging.info(f\"Devices: {df['deviceid'].nunique():,} ({100 * df['deviceid'].nunique() / orig_devices:.2f}%)\")\n",
    "        prev_len = len(df)\n",
    "\n",
    "        # 6) Drop devices with too few points\n",
    "        device_counts = df[\"original_deviceid\"].value_counts()\n",
    "        valid_ids = device_counts[device_counts > MIN_POINTS_PER_DEVICE].index\n",
    "        df = df[df[\"original_deviceid\"].isin(valid_ids)].reset_index(drop=True)\n",
    "        step_dropped = prev_len - len(df)\n",
    "        logging.info(f\"After min-points-per-device filter: {step_dropped:,} dropped ({100 * step_dropped / orig:.2f}%)\")\n",
    "        logging.info(f\"Devices: {df['deviceid'].nunique():,} ({100 * df['deviceid'].nunique() / orig_devices:.2f}%)\")\n",
    "        \n",
    "        # Cleanup and save\n",
    "        df.drop(columns=[\"lat_rad\", \"lon_rad\", \"original_deviceid\"], inplace=True)\n",
    "        out_path = DST_DIR / fp.name\n",
    "        df.to_parquet(out_path, index=False, compression=\"snappy\")\n",
    "        logging.info(f\"Wrote {out_path}\")\n",
    "\n",
    "        # Final summary\n",
    "        final_rows = len(df)\n",
    "        total_dropped = orig - final_rows\n",
    "        logging.info(f\"Total dropped: {total_dropped:,} of {orig:,} ({100 * total_dropped / orig:.2f}% removed)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff07fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
