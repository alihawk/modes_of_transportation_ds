{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4ca348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Sampled and saved: 2023-03-27.parquet\n",
      "✔ Sampled and saved: 2023-03-28.parquet\n",
      "✔ Sampled and saved: 2023-03-29.parquet\n",
      "✔ Sampled and saved: 2023-03-30.parquet\n",
      "✔ Sampled and saved: 2023-03-31.parquet\n",
      "✔ Sampled and saved: 2023-04-01.parquet\n",
      "✔ Sampled and saved: 2023-04-02.parquet\n",
      "❌ Error with processed_regional.parquet: No match for FieldRef.Name(startTs) in DateTime: timestamp[ns]\n",
      "Number_OA: double\n",
      "Number_BUS: double\n",
      "Number_LT: double\n",
      "Number_ST: double\n",
      "Number_TT: double\n",
      "Number_TP: double\n",
      "Number_TPP: double\n",
      "PovpSpeed: double\n",
      "MinSpeed: double\n",
      "MaxSpeed: double\n",
      "Occupancy: double\n",
      "groupKey: string\n",
      "geometry: binary\n",
      "__fragment_index: int32\n",
      "__batch_index: int32\n",
      "__last_in_fragment: bool\n",
      "__filename: string\n",
      "\n",
      "✅ Final unified file saved to: data\\validation_set\\regional_roads\\processed_regional.parquet\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "regional_dir = Path(\"data/validation_set/regional_roads\")\n",
    "parquet_files = list(regional_dir.glob(\"*.parquet\"))\n",
    "output_path = Path(\"data/validation_set/regional_roads/processed_regional.parquet\")\n",
    "output_dir = regional_dir / \"sampled\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rename_map = {\n",
    "    'startTs': 'DateTime',\n",
    "    'Stevilo_OA_sum': 'Number_OA',\n",
    "    'Stevilo_BUS_sum': 'Number_BUS',\n",
    "    'Stevilo_LT_sum': 'Number_LT',\n",
    "    'Stevilo_ST_sum': 'Number_ST',\n",
    "    'Stevilo_TT_sum': 'Number_TT',\n",
    "    'Stevilo_TP_sum': 'Number_TP',\n",
    "    'Stevilo_TPP_sum': 'Number_TPP',\n",
    "    'PovpHitrost_avg': 'PovpSpeed',\n",
    "    'MinHitrost_avg': 'MinSpeed',\n",
    "    'MaxHitrost_avg': 'MaxSpeed',\n",
    "    'Zasedenost_avg': 'Occupancy'\n",
    "}\n",
    "\n",
    "columns_to_keep = list(rename_map.keys()) + ['groupKey', 'geometry']\n",
    "\n",
    "samples = []\n",
    "for f in parquet_files:\n",
    "    try:\n",
    "        gdf = gpd.read_parquet(f, columns=[col for col in columns_to_keep if col != 'TrafficStatus'])\n",
    "        if 'TrafficStatus' in gdf.columns:\n",
    "            gdf['TrafficStatus'] = gdf['TrafficStatus']\n",
    "        gdf = gdf.rename(columns=rename_map)\n",
    "        gdf['DateTime'] = pd.to_datetime(gdf['DateTime'], dayfirst=True, errors='coerce')\n",
    "        gdf = gdf.dropna(subset=['DateTime'])\n",
    "\n",
    "        sample = gdf.sample(n=min(1000, len(gdf)), random_state=42)\n",
    "        samples.append(sample)\n",
    "\n",
    "        sample.to_parquet(output_dir / f.name.replace(\".parquet\", \"_sampled.parquet\"), index=False)\n",
    "        print(f\"✔ Sampled and saved: {f.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {f.name}: {e}\")\n",
    "\n",
    "if samples:\n",
    "    pd.concat(samples, ignore_index=True).to_parquet(output_path, index=False)\n",
    "    print(f\"\\n✅ Final unified file saved to: {output_path}\")\n",
    "    del samples\n",
    "else:\n",
    "    print(\"⚠ No valid samples collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee51fcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 243/243 [09:29<00:00,  2.34s/files]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.5 MiB for an array with shape (2, 2389203) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Combine and save if we got anything\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_highways:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     combined_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_highways\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     output_path = highway_dir / \u001b[33m\"\u001b[39m\u001b[33mprocessed_highways.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     combined_df.to_parquet(output_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    187\u001b[39m     fastpath = blk.values.dtype == values.dtype\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     values = \u001b[43m_concatenate_join_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     fastpath = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fastpath:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:486\u001b[39m, in \u001b[36m_concatenate_join_units\u001b[39m\u001b[34m(join_units, copy)\u001b[39m\n\u001b[32m    483\u001b[39m     concat_values = ensure_block_shape(concat_values, \u001b[32m2\u001b[39m)\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     concat_values = \u001b[43mconcat_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m empty_dtype != empty_dtype_future:\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m empty_dtype == concat_values.dtype:\n\u001b[32m    490\u001b[39m         \u001b[38;5;66;03m# GH#39122, GH#40893\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jposo\\anaconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\dtypes\\concat.py:78\u001b[39m, in \u001b[36mconcat_compat\u001b[39m\u001b[34m(to_concat, axis, ea_compat_axis)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np.ndarray):\n\u001b[32m     77\u001b[39m     to_concat_arrs = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[np.ndarray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_concat_arrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m to_concat_eas = cast(\u001b[33m\"\u001b[39m\u001b[33mSequence[ExtensionArray]\u001b[39m\u001b[33m\"\u001b[39m, to_concat)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ea_compat_axis:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# We have 1D objects, that don't support axis keyword\u001b[39;00m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 36.5 MiB for an array with shape (2, 2389203) and data type object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to all highway xlsx files\n",
    "highway_dir = Path(\"data/validation_set/highways\")\n",
    "xlsx_files = list(highway_dir.glob(\"*.xlsx\"))\n",
    "\n",
    "# Container for processed DataFrames\n",
    "processed_highways = []\n",
    "\n",
    "# Iterate through each file and process\n",
    "for file in tqdm(xlsx_files, desc=\"Loading files\", unit=\"files\"):\n",
    "    try:\n",
    "        # Read data starting from row 4 (header is at row 3)\n",
    "        df = pd.read_excel(file, sheet_name=\"Seznam\", header=3)\n",
    "\n",
    "        # Rename the timestamp column and parse it\n",
    "        df = df.rename(columns={\n",
    "                df.columns[0]: \"Date\",\n",
    "                df.columns[1]: \"Time\"\n",
    "            })\n",
    "        df[\"DateTime\"] = pd.to_datetime(\n",
    "                        df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str),\n",
    "                        format=\"%d.%m.%Y %H:%M\",  # adjust if needed\n",
    "                        errors=\"coerce\"\n",
    "                    )\n",
    "        df = df.drop(columns=[\"Date\", \"Time\"])\n",
    "        # Add filename as station ID\n",
    "        df[\"station_file\"] = file.name\n",
    "\n",
    "        # Store it\n",
    "        processed_highways.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error with {file.name}: {e}\")\n",
    "\n",
    "# Combine and save if we got anything\n",
    "if processed_highways:\n",
    "    combined_df = pd.concat(processed_highways, ignore_index=True)\n",
    "    output_path = highway_dir / \"processed_highways.parquet\"\n",
    "    combined_df.to_parquet(output_path, index=False)\n",
    "    output_path\n",
    "else:\n",
    "    \"⚠ No valid highway files were processed.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
